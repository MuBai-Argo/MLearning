# 监督学习模型

******



[toc]



## 正则化线性回归模型

### 岭回归模型

岭回归的预测公式与最小二乘法相同，但在岭回归中，对权重的选择不仅要在训练数据上得到好的预测结果，还要==拟合附加约束==，权重被期望接近于0。即每个特征对输出的结果应尽可能地小，但是同时仍给出很好的预测结果，这种约束被称为==正则化==。正则化对模型做显示约束，避免过拟合。岭回归用到的这种被称为L2正则化。

岭回归用``linear_model.Ridge``进行实现，降低拟合程度，来提升模型泛化能力。简单性与训练集性能可以通过Ridge的==超参数alpha==进行设置（默认alpha = 1.0）增大alpha会降低训练集性能，提高泛化性能。

```python
ridge = Ridge(alpha = 1.0)
```

当alpha等于0时，岭回归与一般线性回归无异。

> 模型参数用ridge.coef_进行调用，扰动项用ridge.intercept\_进行调用



### Lasso模型

lasso是应用了L1正则化的模型，原理与岭回归一致，但是L1正则化的结果是某些系数刚好等于0，相当于人为消除了某些特征的影响。

lasso模型同样可以通过调节正则化参数`alpha`来时控制模型拟合强度。同时还可以通过设置`max_iter`参数，来设置训练模型的最大次数。

```python
lasso = Lasso(alpha=0.01, max_iter=100000)
```



在实践中，两个模型一般首选岭回归。当特征特别多，但是已知只有几个最重要时，可以选用Lasso模型。也可以适用ElasticNet模型，结合了Lasso和岭回归两种惩罚项。



## 用于分类的线性模型

用于分类的线性模型，决策边界是输入的线性函数。

常见的两种线性分类模型包括Logistic模型和线性支持向量机模型，分别在`linear_model.LogisticRegression`和`svm.LinearSVC`中实现。

两个模型都默认适用L2正则化。对二者来说，决定正则化强度的权衡参数称为C，C值愈大，正则化强度愈弱，拟合程度愈高。

### Logistic模型

```python
X_train, X_test, 
y_train, y_test = train_test_split(X, Y, stratify = Y,# 依据原数据Y中各类比例分配给训练集和验证集，使各类数据比例与原数据集一致。
                                   random_state=42)
logre = LogisticRegression(C=1, penalty="l1"	# penalty参数用于设置正则化方式，默认为l2 
                          ).fit(X_train, Y_train)
logreg.score(X_train, y_train)
logreg.score(X_test, y_test)

```



### linear_svm模型

线性支持向量机模型可以直接处理多分类问题。

```python
linear_svm = Linear_SVC.fit(X, y)
```



## 朴素贝叶斯分类器

朴素贝叶斯分类器与线性模型非常相似，训练速度往往更快，但泛化能力比线性分类器稍差。

朴素贝叶斯通过单独查看每个特征来学习参数，并从每个特征中收集简单对类别统计数据。

sklearn中实现了3中朴素贝叶斯分类器：`Gaussian模型` `BernoulliNB模型` `MultinomialNB模型`。

GaussianNB模型可应用于任意连续数据，BernoulliNB模型可用于二分类数据，MultinomialNB模型适用于计数数据。（BernoulliNB和MultinomialNB模型主要用于文本数据分类）

BernoulliNB分类器计算每个类别中每个特征不为零的个数。

MultinomialNB计算每个类别中每个特征的平均值。

MultinomialNB计算每个类别中每个特征的平均值和标准差。

要想做出预测，需要将数据点和每个类别的统计数据进行比较并将最匹配的类别作为预测结果。

MultinomialNB和BernoulliNB都只有一个参数alpha，用于控制模型复杂度。

GaissoanNB主要用于高维数据。



## 决策树

决策树通过学习一系列的if/else问题是我们最快速度进行分类。数据通常不具有二元特征形式，而是表示为连续特征，为了构造决策树，算法搜遍所有可能的测试，找出对目标变量来说信息量最大的哪一个。对数据反复进行递归划分，知道划分后数据的每个区域都只包含单一目标值，如果某个叶结点所包含的数据点的目标值都相同，称此叶结点为pure。

要对新数据进行分类，首先查看这个点位于特征空间的哪个区域，然后将该区域的多数目标值作为预测结果。

### 控制决策树的复杂度

如果使所有叶结点都为pure，将导致过拟合。

防止过拟合有两种常见的策略，一种是==预剪枝==，另一种使==后剪枝==（先构造树，但随后删除或者折叠信息量很少的节点）

预剪枝的显示条件包括限制树的最大声都、限制叶结点的最大数目，规定一个结点中数据点的最小数目。

决策树通过sklearn分别在`DecisionTreeRegressor`类和`DecisionTreeClassifier`类中实现，sklearn采取的都是预剪枝，没有对后剪枝进行实现。

```python
tree = DecisionTreeClassifier(random_state=0, max_depth=4
                             ).fit(X_train, Y_train)
```

固定树的random_state用于在内部解决平局问题。

通过设置max_depth来限制树的深度

### 对决策树进行可视化

利用tree模块的`export_graphviz`函数来将树进行可视化处理。

函数会输出一个.dot文件格式的文件，同于保存图像的文本文件格式。设置为为节点添加颜色的选项，颜色表示每个结点中的多数类别。

```python
from sklearn.tree import export_graphviz
export_graphviz(tree, out_file = "tree.dot", class_names = ["malignant", "benign"], feature_names=cancer.feature_names, impurity=False, filled=True)
# 可以利用graphviz模块对.dot文件进行读取。
import graphviz
with open("tree.dot") as f:
    dot_graph = f.read()
    graphviz.Source(dot_graph)
# 或者直接调用display函数对树形态进行显示
display(tree)
```

### 树的特征重要性

可以利用树的特征重要性来为每个特征对数的决策的重要性进行排序，对每个特征来说它都是一个介于0到1间的值，特征重要性的求和为0。

利用`tree.feature_importances`可以对特征重要性进行查看，将按照特征顺序输出一个序列。

如果某个特征的feature_importance很小，不能说明这个特征没提供任何特征，这只能说明这个特征没有被树选中，可能是因为另一个特征也包含了相同的信息。



### 决策树集成

通过合并多个决策树模型来构建更强大的模型的方法。

#### 随机森林模型

随机森林模型可以解决决策树对训练数据过拟合的问题。随机森林本质上是许多决策树的集合，其中每棵树都和其他书略有不同，随机森林背后的思想在于，每棵树的预测结果可能都相对较好，但可能对部分数据过拟合，如果构造很多书，并且每棵树的预测结果都很好但是以不同的方式过拟合，那么我们就可以对这些树的结果取平均值来降低过拟合程度，既能减少过拟合又能保持树的预测能力。

随机森林中书的随机化方法有两种，一种是通过选择用于构造树的数据点，另一种是通过选择每次划分测试的特征。

##### 构造随机森林

首先需要确定用于构造的树的个数，设置`n_estimator参数`。

不同的树在构造时彼此完全独立，算法对每棵树进行不同的随机选择，以确保树和树之间的区别。

首先要对数据进行==自主采样(bootstrap)==，即从n_sample各数据点中有放回的重复随机抽取一个样本，共抽取n_sample此，从而创建一个与愿数据集大小相同的新数据集，但新数据集中会存在部分数据缺失。

基于新创建的数据集，算法在每个结点处随机选择特征的一个子集，并对其中一个特征寻找最优测试，而不是对每个结点都寻找最佳测试。选择特征的个数有`max_feature参数`进行控制。每个结点中特征子集的个数的选择是相互独立的，这样树的每个节点可以适用不同子集来做出决策。

在自主采样的基础下，随机森林构造每一个决策树的数据集都略有不同，每棵树中的每次划分都是基于特征的的不同子集。

想要利用随机森林进行预测，算法首先对森林中每棵树进行预测，对于回归问题，可以取结果的平均值作为最终预测结果，对于分类问题采用软投票策略。

```python
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimator=5,# 树的数目
                                random_state=2)
forest.fit(X_train, Y_train)

```

作为随机森林的一部分，决策树被保存在estimator属性中，通过将每棵树学到的决策边界可视化，从而将总预测可视化。

与单棵树相比，随机森林中有更多特征的重要性不为0。

用于回归和分类的随机森林是目前应用最广泛的机器学习方法之一。



#### 梯度提升决策树

梯度提升回归树是另一种集成方法，通过合并多个决策树来构建一个更为强大的模型，采取连续的方式构造树，每棵树都视图纠正前一棵树的错误。默认情况下，梯度提升决策树中没有随机化，而是使用强预剪枝。

梯度提升树使用的深度通常很小（一般不超过5）。

与随机森林相比，梯度提升树通常对参数设置更为敏感。

```python
gbrt = GradientBoostionClassifier(random_state=0, max_depth=3, learning_rate = 0.01)
gbrt.fit(X_train, y_train)
```

为了降低过拟合程度，我们可以通过限制最大深度来加强预剪枝，或者降低学习率。

由于梯度提升决策树和随机森林模型两种方法在类似的数据上表现的都很好，一个常用的方法是先尝试鲁棒性较号的随机森林。如果随机森林效果很好但预测时间太长，或者是机器学习模型精度小数点后第二位的提升也很重要，则切换成梯度提升。

## 核支持向量机

核支持向量机通常简称为SVM。

像数据表中添加非线性特征可以让线性模型变得更加强大。

但是通常来说，我们并不知道要添加哪些特征，并且添加许多特征的计算开销可能会很大，使用==核技巧==可以让我们在更高维空间中学习分类器，而不用实际计算可能非常大的数据表示。其原理是直接计算拓展特征表示中数据点之间的距离（内积），而不用对实际拓展进行计算。

对于SVM有两种方式将数据映射到更高维的空间中，一种是==多项式核==，在一定阶数中计算与那是特征所有可能的多项式。另一种是==径向基函数核（高斯核）==。

位于决策边界上的那些点称为支持向量。要对新样本点进行预测需要测量它与每个支持向量之间的距离。分类决策是基于它与支持向量之间的距离以及在训练过程中学到的支持向量重要性（dual_coef属性）来做出的。

数据点之间的距离由高斯核给出:
$$
k_{rbf}(x_1, x_2) = exp(-\gamma||x_1-x_2||^2)\\
||x_1-x_2||表示欧氏距离\\
\gamma表示控制高斯核宽度的参数
$$

```python
from sklearn.svm import SVC
svm = SVC(kernel = "rbf",	# 表示采用高斯核方式
         C = 10, gamma = 0.1).fit(X, y)
mglearn.plots.plot_2d_separator(svm, X, eps=.5)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
sv = svm.support_vectors# 绘制支持向量
sv_label = svm.dual_coef_.ravel()>0
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels. s=15, markeredgewidth = 3)

```

gamma参数用于控制高斯核的宽度，决定了点与点之间靠近的距离。C参数是正则化参数。

SV,对参数的设定核数据的缩放十分敏感。要求所有特征有着相似的变化范围。

所以，需要对数据进行预处理，将数据归一化。

### SVM的优缺点

SVM允许决策边界很复杂，及时数据只有几个特征。SVM在低维数据和高维数据上表现的都很好，但对样本个数的缩放表现不好，适用于较少数据的模型建立（10000以内）

SVM对参数和数据非常敏感，并且很难检查与解释。

但当所有特征的测量单位相似且范围差不多时SVM值得尝试。

核SVM的重要参数是正则化参数C、核的选择以及与核相关的参数。gamma和C控制的都是模型复杂度。

## 分类器的不确定度估计

分类器能给出预测的不确定估计。

sklearn中有两个函数`decision_function`和`predict_proba`可以用于获取分类器的不确定度估计。

大多数分类器都至少由其中的一个函数。

### 决策函数decision_function

对于二分类情况，decision_function返回值的形状是（n_samples, ）为每个样本都返回一个浮点数

```python
model.decision_function(X_test)
```

对于类别1来说，这个值表示模型对该数据点属于正类的置信程度。

decision_function可以在任意范围内取值，这取决于数据和模型参数，由于可以任意缩放，所以decision_function的输出往往很难解释。

### 预测概率predict_proba

predict_proba的输出是每个类别的概率，相比决策函数来说更加容易理解。

输出矩阵中每行的值表示每个分类结果所占的概率可能性。

```python
model.predict_proba(X_test[:, 6])
```

每一行总和为1。

不确定度大小实际上反映了数据依赖于模型和参数的不确定度。过拟合更强的模型可能会做出置信测度更强的预测，即使预测结果可能是错的。复杂度越低的模型通常对预测的不确定度越大。如果模型给出的不确定度符合实际情况，则这个模型被称为==矫正模型==。

面对新数据集，通常最好先从简单模型开始，看能看到什么样的结果，在对数据有了进一步的了解后，可以考虑用于构造更复杂的算法。